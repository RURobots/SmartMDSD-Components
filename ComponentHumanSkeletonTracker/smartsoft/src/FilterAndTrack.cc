// Code generated by the SmartSoft MDSD Toolchain
// The SmartSoft Toolchain has been developed by:
//  
// Service Robotics Research Center
// University of Applied Sciences Ulm
// Prittwitzstr. 10
// 89075 Ulm (Germany)
//
// Information about the SmartSoft MDSD Toolchain is available at:
// www.servicerobotik-ulm.de
//
// This file is generated once. Modify this file to your needs. 
// If you want the toolchain to re-generate this file, please 
// delete it before running the code generator.
//--------------------------------------------------------------------------
#include "FilterAndTrack.hh"
#include "ComponentHumanSkeletonTracker.hh"

#define RUR_StandAloneMathsLibrary
#include "Vector.h"
#include "Matrix.h"
using namespace RUR_RobotMaths;
#include <iostream>

FilterAndTrack::FilterAndTrack(SmartACE::SmartComponent *comp) 
:	FilterAndTrackCore(comp),
	opWrapper(op::ThreadManagerMode::Asynchronous),
	firstTime(true),
	lastSensorData(),
	a(numberOfStates, numberOfStates),
	c(numberOfMeasurements, numberOfStates),
	q(numberOfMeasurements, numberOfMeasurements),
	r(numberOfStates, numberOfStates),
	z(numberOfMeasurements),							// The measurements are X, Y, Z
	timeStep(0.033),
	measurementVariance(MEASUREMENT_VARIANCE),
	processVariance(PROCESS_VARIANCE),
	newCov(numberOfStates, numberOfStates),
	widthFocalLength(1.0),
	heightFocalLength(1.0),
	widthPricipalAxis(50),
	heightPricipalAxis(50)
{
	std::cout << "constructor FilterAndTrack\n";
}

FilterAndTrack::~FilterAndTrack()
{
	std::cout << "destructor FilterAndTrack\n";
}

void FilterAndTrack::on_RGBDImagePushServiceIn(const DomainVision::CommRGBDImage &input)
{
	int x,y;									// 2D pixel position
	float x3d, y3d, z3d;						// 3D position
	float score;								// Score for 3D position
	DomainVision::DepthFormatType depthFormat;	// The data format of the depth image

	maxNumberOfBodies = COMP->getGlobalState().getConfig().getMaximum_bodies();	// The maximum number of people to detect
	depthRegionSize = COMP->getGlobalState().getConfig().getDepth_region_size();// The size of the region to average for depth
	const float scoreThreshold = COMP->getGlobalState().getConfig().getScore_threshold();	// The score threshold use to determine if joint positition should be used
	const float minimumDistance = COMP->getGlobalState().getConfig().getMinimum_distance();	// The minimum measurement distance
	const float maximumDistance = COMP->getGlobalState().getConfig().getMaximum_distance();	// The maximum measurement distance
	const float maximumVelocity = COMP->getGlobalState().getConfig().getMaximum_velocity();	// The maximum measurement velocity
	const bool displayDebugInfo = COMP->getGlobalState().getSettings().getDebug_info();		// Controls if debug information is printed
	const bool displayImages = COMP->getGlobalState().getSettings().getDisplay_images();	// Controls if colour and depth images are displayed

	try
	{
		// Get camera intrinsics
		widthFocalLength = input.getColor_image().getIntrinsic_mElemAtPos(0);
		heightFocalLength = input.getColor_image().getIntrinsic_mElemAtPos(5);
		widthPricipalAxis = input.getColor_image().getIntrinsic_mElemAtPos(2);
		heightPricipalAxis = input.getColor_image().getIntrinsic_mElemAtPos(6);

		if (displayDebugInfo)
		{
			std::cout << "Camera horizontal focal length: " << widthFocalLength << std::endl;
			std::cout << "Camera horizontal centre: " << widthPricipalAxis << std::endl;
			std::cout << "Camera vertical focal length: " << heightFocalLength << std::endl;
			std::cout << "Camera vertical centre: " << heightPricipalAxis << std::endl;
		}

		// Read in the current colour image and covert to format for OpenPose
		op::Matrix imageToProcess; // Input image in OpenPose format
		cv::Mat cvColourInputImage;	// Input image in CV format
		extractOpenPoseColourImage(input, cvColourInputImage, imageToProcess);

		// Read in the current depth images and covert to OpenCV format
		cv::Mat cvDepthImage;
		depthFormat = extractCvDepthImage(input, cvDepthImage);

		if (displayDebugInfo)
		{
			std::cout << "Depth format:            " << depthFormat << std::endl;
			std::cout << "Colour image dimensions: " << imageToProcess.rows() << "x" << imageToProcess.cols() << std::endl;
			std::cout << "Depth image dimensions:  " << cvDepthImage.rows << "x" << cvDepthImage.cols << std::endl;
		}

		//  Process image
		auto datumProcessed = opWrapper.emplaceAndPop(imageToProcess);

		// Resize the output to contain the required number of BodyData elements
		int maxSize = datumProcessed->at(0)->poseKeypoints.getSize(0);
		if (maxSize > maxNumberOfBodies)
		{
			maxSize = maxNumberOfBodies;
		}
		thisSensorData.resizeBodyData(maxSize);

		// For each joint in each body, convert to 3D and output data
		if (datumProcessed != nullptr)
		{
			std::cout << "People detected: " << datumProcessed->at(0)->poseKeypoints.getSize(0) << std::endl;
			if (displayDebugInfo)
			{
				std::cout << "Number of joints per body: " << datumProcessed->at(0)->poseKeypoints.getSize(1) << std::endl;
			}

			DomainHumanTracking::CommBodyData bodyData;
			bodyData.setIsTracked(false);
			bodyData.resizeJointData(datumProcessed->at(0)->poseKeypoints.getSize(1));
			for (int bodyIndex = 0; (bodyIndex < maxNumberOfBodies) && (bodyIndex < datumProcessed->at(0)->poseKeypoints.getSize(0)); bodyIndex++)
			{
				for (int jointIndex = 0; jointIndex < datumProcessed->at(0)->poseKeypoints.getSize(1); jointIndex++)
				{
					// Convert 2D joint position into 3D position
					x = datumProcessed->at(0)->poseKeypoints.at({bodyIndex, jointIndex, 0});
					y = datumProcessed->at(0)->poseKeypoints.at({bodyIndex, jointIndex, 1});
					score = datumProcessed->at(0)->poseKeypoints.at({bodyIndex, jointIndex, 2});
					convert2dTo3d(x, y, cvDepthImage, depthFormat, x3d, y3d, z3d);

					if (displayDebugInfo)
					{
						std::cout << "Body " << bodyIndex <<", Joint " << jointIndex << ": 2D=(" << x << ", " << y << "), 3D=(" << x3d << ", " << y3d << ", " << z3d << "), score=" << score << std::endl;
					}

					// Put data into output data structure
					DomainHumanTracking::CommJointData jointData;
					if ((score == 0.0) || (z3d < minimumDistance) || (z3d > maximumDistance))
					{
						jointData.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::UNKNOWN);
						DomainHumanTracking::Comm2dVector jointPosition2d;
						jointPosition2d.setX(0.0);
						jointPosition2d.setY(0.0);
						jointData.setJointPosition2D(jointPosition2d);
						jointData.setJointVelocity2D(jointPosition2d);
						DomainHumanTracking::Comm3dVector jointPosition3d;
						jointPosition3d.setX(0.0);
						jointPosition3d.setY(0.0);
						jointPosition3d.setZ(0.0);
						jointData.setJointPosition3d(jointPosition3d);
						jointData.setJointVelocity3d(jointPosition3d);
					}
					else
					{
						if (score > scoreThreshold)
						{
							jointData.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::TRACKED);
						}
						else
						{
							jointData.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::INFERRED);
						}
						DomainHumanTracking::Comm2dVector jointPosition2d;
						jointPosition2d.setX(x);
						jointPosition2d.setY(y);
						jointData.setJointPosition2D(jointPosition2d);
						DomainHumanTracking::Comm3dVector jointPosition3d;
						jointPosition3d.setX(z3d);
						jointPosition3d.setY(-x3d);
						jointPosition3d.setZ(-y3d);
						jointData.setJointPosition3d(jointPosition3d);
						DomainHumanTracking::Comm2dVector jointVelocity2d;
						jointVelocity2d.setX(0.0);
						jointVelocity2d.setY(0.0);
						jointData.setJointVelocity2D(jointVelocity2d);
						DomainHumanTracking::Comm3dVector jointVelocity3d;
						jointVelocity3d.setX(0.0);
						jointVelocity3d.setY(0.0);
						jointVelocity3d.setZ(0.0);
						jointData.setJointVelocity3d(jointVelocity3d);
					}

					bodyData.setJointDataElemAtPos(jointIndex, jointData);

					if (displayDebugInfo)
					{
						std::cout << "Body " << bodyIndex << ", Joint " << jointIndex << ": " << jointData << std::endl;
					}
				} // joint loop

				CommBasicObjects::CommTimeStamp acquisitionTime;
				timeval currentTime;
				gettimeofday(&currentTime, NULL);
				acquisitionTime.set(currentTime.tv_sec, currentTime.tv_usec);
				bodyData.setAcquisitionTime(acquisitionTime);

				bodyData.setIsTracked(true);

				bodyData.setLeftHandState(DomainHumanTracking::HandTrackingStateType::UNKNOWN);
				bodyData.setRightHandState(DomainHumanTracking::HandTrackingStateType::UNKNOWN);

				convertOpenPoseToKinect(bodyData);

				thisSensorData.setBodyDataElemAtPos(bodyIndex, bodyData);
			} // body loop

			// Augment depth image with detected points
			for (int bodyIndex = 0; (bodyIndex < maxNumberOfBodies) && (bodyIndex < datumProcessed->at(0)->poseKeypoints.getSize(0)); bodyIndex++)
			{
				for (int jointIndex = 0; jointIndex < datumProcessed->at(0)->poseKeypoints.getSize(1); jointIndex++)
				{
					// Convert 2D joint position into 3D position
					x = datumProcessed->at(0)->poseKeypoints.at({bodyIndex, jointIndex, 0});
					y = datumProcessed->at(0)->poseKeypoints.at({bodyIndex, jointIndex, 1});
					markDepthImage(x, y, cvDepthImage, depthFormat);
				}
			}

			// Calculate the velocities from the positions
			long long int thisTime = convertToUsecs(thisSensorData.getBodyDataElemAtPos(0).getAcquisitionTime());
			long long int lastTime = convertToUsecs(lastSensorData.getBodyDataElemAtPos(0).getAcquisitionTime());
			calculateVelocitiesKalman(&lastSensorData, &thisSensorData, (thisTime - lastTime) / 1000000.0, maximumVelocity);

			// Output the filtered positions and velocities and save in shared variable
			this->humanSkeletonsPushServiceOutPut(thisSensorData);
			COMP->currentSkeletonsMutex.acquire();
			COMP->currentSkeletons = thisSensorData;
			COMP->currentSkeletonsMutex.release();

			if (displayImages)
			{
				display(datumProcessed, cvDepthImage);
			}

			if (displayDebugInfo)
			{
				std::cout << "Data: " << thisSensorData << std::endl;
			}

			// Update "last" values
			for (int bodyIndex = 0; bodyIndex < thisSensorData.getBodyDataSize(); bodyIndex++)
			{
				lastSensorData.setBodyDataElemAtPos(bodyIndex, thisSensorData.getBodyDataElemAtPos(bodyIndex));
			}
		}

		else
		{
			std::cout << "Image could not be processed." << std::endl;
		}
	}
	catch(...)
	{
		std::cout << "Exception while processing image." << std::endl;
	}
}

int FilterAndTrack::on_entry()
{


	try
    {
         const auto opTimer = op::getTimerInit();

         // Configuring OpenPose
         std::cout << "Configuring OpenPose..." << std::endl;
         op::WrapperStructPose wrapperStructPose{};
         wrapperStructPose.modelFolder = COMP->getGlobalState().getConfig().getModel_folder().c_str();
         wrapperStructPose.netInputSize = {-1,128};
         opWrapper.configure(wrapperStructPose);

         std::cout << "Created OpenPose wrapper" << std::endl;

         // Starting OpenPose
         op::opLog("Starting thread(s)...", op::Priority::High);
         opWrapper.start();

         std::cout << "OpenPose wrapper started" << std::endl;

         lastSensorData.resizeBodyData(maxNumberOfBodies);
		 thisSensorData.resizeBodyData(maxNumberOfBodies);
		 DomainHumanTracking::CommBodyData bodyData;
		 bodyData.setIsTracked(false);
		 bodyData.setAcquisitionTime(CommBasicObjects::CommTimeStamp());
		 bodyData.resizeJointData(numberOfJoints);
		 DomainHumanTracking::CommJointData jointData;
		 jointData.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::UNKNOWN);
		 for (int jointIndex = 0; jointIndex < numberOfJoints; jointIndex++)
		 {
			 bodyData.setJointDataElemAtPos(jointIndex, jointData);
		 }
		 for (int bodyIndex = 0; bodyIndex < maxNumberOfBodies; bodyIndex++)
		 {
			 lastSensorData.setBodyDataElemAtPos(bodyIndex, bodyData);
			 thisSensorData.setBodyDataElemAtPos(bodyIndex, bodyData);
		 }
    }
    catch (const std::exception&)
    {
    	std::cout << "Exception caught during execution of on_entry method" << std::endl;
    	return -1;
    }

    return 0;
}

int FilterAndTrack::on_execute()
{
	// this method is called from an outside loop,
	// hence, NEVER use an infinite loop (like "while(1)") here inside!!!
	// also do not use blocking calls which do not result from smartsoft kernel
	
	// to get the incoming data, use this methods:
	Smart::StatusCode status;
	DomainVision::CommRGBDImage rGBDImagePushServiceInObject;
	status = this->rGBDImagePushServiceInGetUpdate(rGBDImagePushServiceInObject);
	if(status == Smart::SMART_OK)
	{
	}
	else
	{
	}

	return 0;
}

int FilterAndTrack::on_exit()
{
	// No resources to cleanup

	return 0;
}

void FilterAndTrack::extractOpenPoseColourImage(const DomainVision::CommRGBDImage& input, cv::Mat& cvImage, op::Matrix& openPoseImage)
{
	unsigned int width, height, size;	// The width, height and pixel data size of the colour image

	try
	{
		DomainVision::CommVideoImage colourImage = input.getColor_image();
		((DomainVision::CommRGBDImage)input).get_color_image_size(width, height, size);
		cv::cvtColor(cv::Mat(height, width, CV_8UC3, (void*)colourImage.get_data()), cvImage, CV_RGB2BGR);
		openPoseImage = OP_CV2OPCONSTMAT(cvImage);
	}
	catch(...)
	{
		std::cout << "Error extracting colour image from input" << std::endl;
		cvImage = cv::Mat::zeros(100, 100, CV_8UC3);
		openPoseImage = OP_CV2OPCONSTMAT(cvImage);
	}
}

DomainVision::DepthFormatType FilterAndTrack::extractCvDepthImage(const DomainVision::CommRGBDImage& input, cv::Mat& openCvImage)
{
	unsigned int depthWidth, depthHeight, depthSize;
	DomainVision::DepthFormatType depthFormat;

	depthWidth = input.getDepth_image().getWidth();
	depthHeight = input.getDepth_image().getHeight();
	depthSize = input.getDepth_image().getDataSize();
	depthFormat = input.getDepth_image().getFormat();

	try
	{
		switch(depthFormat)
		{
			case DomainVision::DepthFormatType::DOUBLE:
				openCvImage = cv::Mat(depthHeight, depthWidth, CV_64FC1, (void*)input.getDepth_image().get_distances());
				break;
			case DomainVision::DepthFormatType::FLOAT:
				openCvImage = cv::Mat(depthHeight, depthWidth, CV_32FC1, (void*)input.getDepth_image().get_distances());
				break;
			case DomainVision::DepthFormatType::UINT16:
				openCvImage = cv::Mat(depthHeight, depthWidth, CV_16UC1, (void*)input.getDepth_image().get_distances());
				break;
			case DomainVision::DepthFormatType::UINT8:
			default:
				openCvImage = cv::Mat(depthHeight, depthWidth, CV_8UC1, (void*)input.getDepth_image().get_distances());
				break;
		}
	}
	catch(...)
	{
		std::cout << "Error extracting depth image from input." << std::endl;
		openCvImage = cv::Mat::zeros(100, 100, CV_8UC1);
		depthFormat = DomainVision::DepthFormatType::UINT8;
	}

	return depthFormat;
}

void FilterAndTrack::convert2dTo3d(int x, int y, cv::Mat& cvDepthImage, DomainVision::DepthFormatType depthFormat, float& x3d, float& y3d, float& z3d)
{
	try
	{
		// Determine the associated depth value in metres from the depth image
		switch(depthFormat)
		{
			case DomainVision::DepthFormatType::DOUBLE:
				z3d = findMinimumDepth<double>(x, y, cvDepthImage);
				break;
			case DomainVision::DepthFormatType::FLOAT:
				z3d = findMinimumDepth<float>(x, y, cvDepthImage);
				break;
			case DomainVision::DepthFormatType::UINT16:
				z3d = findMinimumDepth<unsigned short>(x, y, cvDepthImage) / 1000.0;
				break;
			case DomainVision::DepthFormatType::UINT8:
				z3d = findMinimumDepth<unsigned char>(x, y, cvDepthImage) / 1000.0;
				break;
		}

		// Caluclate the 3D x and y positions based on the depth
		x3d = z3d * ((x - widthPricipalAxis) / widthFocalLength);
		y3d = z3d * ((y - heightPricipalAxis) / heightFocalLength);
	}
	catch(...)
	{
		std::cout << "Error converting 2D pixel position to 3D position." << std::endl;
		x3d = y3d = z3d = 0.0;
	}
}

void FilterAndTrack::markDepthImage(int x, int y, cv::Mat& cvDepthImage, DomainVision::DepthFormatType depthFormat)
{
	try
	{
		// Augment the depth image with the joint position marks
		const int SIZE = 5;
		if ((x > 0) && (x < cvDepthImage.cols - 1) && (y > 0) && (y < cvDepthImage.rows - 1))
		{
			for (int j = x - SIZE; j < x + SIZE + 1; j++)
			{
				for (int k = y - SIZE; k < y+ SIZE + 1; k++)
				{
					switch(depthFormat)
					{
						case DomainVision::DepthFormatType::DOUBLE:
							cvDepthImage.at<double>(cv::Point(j, k)) = 0.0;
							break;
						case DomainVision::DepthFormatType::FLOAT:
							cvDepthImage.at<float>(cv::Point(j, k)) = 0.0;
							break;
						case DomainVision::DepthFormatType::UINT16:
							cvDepthImage.at<unsigned short>(cv::Point(j, k)) = 65535;
							break;
						case DomainVision::DepthFormatType::UINT8:
							cvDepthImage.at<unsigned char>(cv::Point(j, k)) = 255;
							break;
					}
				}
			}
		}
	}
	catch(...)
	{
		std::cout << "Error 3D position in depth image." << std::endl;
	}
}

void FilterAndTrack::convertOpenPoseToKinect(DomainHumanTracking::CommBodyData& bodyData)
{
	DomainHumanTracking::CommBodyData newBodyData(bodyData);

	if (bodyData.getIsTracked())
	{
		// Shuffle the joints
		for (int kinectJointIndex = 0; kinectJointIndex < numberOfJoints; kinectJointIndex++)
		{
			switch (kinectJointIndex)
			{
				case KinectSpineBase:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpSpineBase));
					break;
				}
				case KinectSpineMid:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpSpineBase));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpSpineBase),
							bodyData.getJointDataElemAtPos(OpSpineShoulder),
							0.5
						)
					);
					break;
				}
				case KinectNeck:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpHead));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpSpineShoulder),
							bodyData.getJointDataElemAtPos(OpHead),
							0.2
						)
					);
					break;
				}
				case KinectHead:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpHead));
					break;
				}
				case KinectShoulderLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpShoulderLeft));
					break;
				}
				case KinectElbowLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpElbowLeft));
					break;
				}
				case KinectWristLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristLeft));
					break;
				}
				case KinectHandLeft:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristLeft));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpElbowLeft),
							bodyData.getJointDataElemAtPos(OpWristLeft),
							1.2
						)
					);
					break;
				}
				case KinectShoulderRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpShoulderRight));
					break;
				}
				case KinectElbowRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpElbowRight));
					break;
				}
				case KinectWristRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristRight));
					break;
				}
				case KinectHandRight:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristRight));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpElbowRight),
							bodyData.getJointDataElemAtPos(OpWristRight),
							1.2
						)
					);
					break;
				}
				case KinectHipLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpHipLeft));
					break;
				}
				case KinectKneeLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpKneeLeft));
					break;
				}
				case KinectAnkleLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpAnkleLeft));
					break;
				}
				case KinectFootLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpFootLeft));
					break;
				}
				case KinectHipRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpHipRight));
					break;
				}
				case KinectKneeRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpKneeRight));
					break;
				}
				case KinectAnkleRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpAnkleRight));
					break;
				}
				case KinectFootRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpFootRight));
					break;
				}
				case KinectSpineShoulder:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpSpineShoulder));
					break;
				}
				case KinectHandTipLeft:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristLeft));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpElbowLeft),
							bodyData.getJointDataElemAtPos(OpWristLeft),
							1.3
						)
					);
					break;
				}
				case KinectThumbLeft:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristLeft));
					break;
				}
				case KinectHandTipRight:
				{
//					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristRight));
					newBodyData.setJointDataElemAtPos(
						kinectJointIndex,
						interpolateJoint(
							bodyData.getJointDataElemAtPos(OpElbowRight),
							bodyData.getJointDataElemAtPos(OpWristRight),
							1.3
						)
					);
					break;
				}
				case KinectThumbRight:
				{
					newBodyData.setJointDataElemAtPos(kinectJointIndex, bodyData.getJointDataElemAtPos(OpWristRight));
					break;
				}
			}
		}

		// Output the data
		for (int jointIndex = 0; jointIndex < numberOfJoints; jointIndex++)
		{
			bodyData.setJointDataElemAtPos(jointIndex, newBodyData.getJointDataElemAtPos(jointIndex));
		}
	}
}

DomainHumanTracking::CommJointData FilterAndTrack::interpolateJoint(DomainHumanTracking::CommJointData firstJoint, DomainHumanTracking::CommJointData secondJoint, double scale)
{
	DomainHumanTracking::CommJointData newJoint;

	newJoint.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::UNKNOWN);

	DomainHumanTracking::Comm2dVector jointPosition2D({0.0, 0.0});
	DomainHumanTracking::Comm2dVector jointVelocity2D({0.0, 0.0});
	DomainHumanTracking::Comm3dVector jointPosition3D({0.0, 0.0, 0.0});
	DomainHumanTracking::Comm3dVector jointVelocity3D({0.0, 0.0, 0.0});

	newJoint.setJointPosition2D(jointPosition2D);
	newJoint.setJointVelocity2D(jointVelocity2D);
	newJoint.setJointPosition3d(jointPosition3D);
	newJoint.setJointVelocity3d(jointVelocity3D);

	if ((firstJoint.getIsJointTracked() == DomainHumanTracking::JointTrackingStateType::TRACKED) && (secondJoint.getIsJointTracked() == DomainHumanTracking::JointTrackingStateType::TRACKED))
	{
		newJoint.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::TRACKED);

		jointPosition3D.setX(
			firstJoint.getJointPosition3d().getX() +
			(secondJoint.getJointPosition3d().getX() - firstJoint.getJointPosition3d().getX()) * scale
		);
		jointPosition3D.setY(
			firstJoint.getJointPosition3d().getY() +
			(secondJoint.getJointPosition3d().getY() - firstJoint.getJointPosition3d().getY()) * scale
		);
		jointPosition3D.setZ(
			firstJoint.getJointPosition3d().getZ() +
			(secondJoint.getJointPosition3d().getZ() - firstJoint.getJointPosition3d().getZ()) * scale
		);

		newJoint.setJointPosition3d(jointPosition3D);

		jointVelocity3D.setX(
			firstJoint.getJointVelocity3d().getX() +
			(secondJoint.getJointVelocity3d().getX() - firstJoint.getJointVelocity3d().getX()) * scale
		);
		jointVelocity3D.setY(
			firstJoint.getJointVelocity3d().getY() +
			(secondJoint.getJointVelocity3d().getY() - firstJoint.getJointVelocity3d().getY()) * scale
		);
		jointVelocity3D.setZ(
			firstJoint.getJointVelocity3d().getZ() +
			(secondJoint.getJointVelocity3d().getZ() - firstJoint.getJointVelocity3d().getZ()) * scale
		);

		newJoint.setJointVelocity3d(jointVelocity3D);
	}

	return newJoint;
}

void FilterAndTrack::calculateVelocitiesKalman(DomainHumanTracking::CommHumanPositionsAndVelocities* lastSensorData, DomainHumanTracking::CommHumanPositionsAndVelocities* thisSensorData, float timeDifference, float maximumVelocity)
{
	if (firstTime)
	{
		// Initialise Kalman Matrices
		a = Matrix::Identity(numberOfStates, numberOfStates);
		a[0][3] = timeStep;
		a[1][4] = timeStep;
		a[2][5] = timeStep;

		c = Matrix::Zeros(numberOfMeasurements, numberOfStates);
		c[0][0] = 1;
		c[1][1] = 1;
		c[2][2] = 1;

		q = Matrix::Identity(numberOfMeasurements, numberOfMeasurements) * measurementVariance;

		r = Matrix::Zeros(numberOfStates, numberOfStates);
		r[3][3] = processVariance;
		r[4][4] = processVariance;
		r[5][5] = processVariance;

		for (int bodyNumber = 0; (bodyNumber < maxNumberOfBodies) && (bodyNumber < maxNumberOfBodies); bodyNumber++)
		{
			for (int jointNumber = 0; jointNumber < numberOfJoints; jointNumber++)
			{
				x[bodyNumber][jointNumber] = new Vector(numberOfStates);
				*x[bodyNumber][jointNumber] = Vector::Zeros(numberOfStates);
				if ((thisSensorData->getBodyDataElemAtPos(bodyNumber).getIsTracked()) && (thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getIsJointTracked() == DomainHumanTracking::JointTrackingStateType::TRACKED))
				{
					x[bodyNumber][jointNumber]->SetValue(0, thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getX());
					x[bodyNumber][jointNumber]->SetValue(1, thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getY());
					x[bodyNumber][jointNumber]->SetValue(2, thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getZ());
					x[bodyNumber][jointNumber]->SetValue(3, 0.0);
					x[bodyNumber][jointNumber]->SetValue(4, 0.0);
					x[bodyNumber][jointNumber]->SetValue(5, 0.0);
				}
				else
				{
					x[bodyNumber][jointNumber]->SetValue(0, 0.0);
					x[bodyNumber][jointNumber]->SetValue(1, 0.0);
					x[bodyNumber][jointNumber]->SetValue(2, 0.0);
					x[bodyNumber][jointNumber]->SetValue(3, 0.0);
					x[bodyNumber][jointNumber]->SetValue(4, 0.0);
					x[bodyNumber][jointNumber]->SetValue(5, 0.0);
				}

				cov[bodyNumber][jointNumber] = new Matrix(numberOfStates, numberOfStates);
				*cov[bodyNumber][jointNumber] = Matrix::Identity(numberOfStates, numberOfStates);

				filter[bodyNumber][jointNumber] = new KalmanFilterWithoutControls(a, c, q, r);
				if (filter[bodyNumber][jointNumber] == 0)
				{
					printf("Error\n");
				}

				lastSensorData->setBodyDataElemAtPos(bodyNumber, thisSensorData->getBodyDataElemAtPos(bodyNumber));

				lastMeasurementTime[bodyNumber][jointNumber] = convertToUsecs(thisSensorData->getBodyDataElemAtPos(0).getAcquisitionTime()) / 1000000.0;

				// Initialise the covariance matrix
				(*x[bodyNumber][jointNumber])[0] = 0.0;
				(*x[bodyNumber][jointNumber])[1] = 0.0;
				(*x[bodyNumber][jointNumber])[2] = 0.0;
				(*x[bodyNumber][jointNumber])[3] = 0.0;
				(*x[bodyNumber][jointNumber])[4] = 0.0;
				(*x[bodyNumber][jointNumber])[5] = 0.0;
				z[0] = 0.0;
				z[1] = 0.0;
				z[2] = 0.0;
				for (int i = 0; i <  100; i++)
				{
					*x[bodyNumber][jointNumber] = filter[bodyNumber][jointNumber]->Update(*x[bodyNumber][jointNumber], *cov[bodyNumber][jointNumber], true, z, true, newCov);
					*cov[bodyNumber][jointNumber] = newCov;
				}
			}
		}

#ifndef RUR_StandAloneMathsLibrary
		logger->LOG_ERROR(ErrorLogger::Informational, "Kinect Sensor has finished pre-calculating Kalman covariance matrix");
#endif

	}
	firstTime = false;

	for (int bodyNumber = 0; (bodyNumber < maxNumberOfBodies) && (bodyNumber < thisSensorData->getBodyDataSize()); bodyNumber++)
	{
		for (int jointNumber = 0; jointNumber < numberOfJoints; jointNumber++)
		{
			if (
				(thisSensorData->getBodyDataElemAtPos(bodyNumber).getIsTracked() == true) &&
				(lastSensorData->getBodyDataElemAtPos(bodyNumber).getIsTracked() == true) &&
				(thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getIsJointTracked() == DomainHumanTracking::JointTrackingStateType::TRACKED) &&
				(lastSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getIsJointTracked() == DomainHumanTracking::JointTrackingStateType::TRACKED)
			)
			{
				// Iterate over the required number of timeSteps using interpolated data
				double currentTime = convertToUsecs(thisSensorData->getBodyDataElemAtPos(0).getAcquisitionTime()) / 1000000.0;
				double timeTotalDuration = currentTime - lastMeasurementTime[bodyNumber][jointNumber];
				double numberOfSteps = round(timeTotalDuration / timeStep);

				if ((numberOfSteps < 1) || (numberOfSteps > 5))
				{
					// Reinitialise
					numberOfSteps = 1;
					(*x[bodyNumber][jointNumber])[0] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getX();
					(*x[bodyNumber][jointNumber])[1] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getY();
					(*x[bodyNumber][jointNumber])[2] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getZ();
					(*x[bodyNumber][jointNumber])[3] = 0.0;
					(*x[bodyNumber][jointNumber])[4] = 0.0;
					(*x[bodyNumber][jointNumber])[5] = 0.0;
				}

				for (int i = 0; i < numberOfSteps; i++)
				{
					if (i == (numberOfSteps - 1))
					{
						// Determine the X,Y and Z components measurement vector, interpolating as necessary
						z[0] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getX();
						z[1] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getY();
						z[2] = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointPosition3d().getZ();

						// Perform the full Kalman update step using the measured data
						*x[bodyNumber][jointNumber] = filter[bodyNumber][jointNumber]->Update(*x[bodyNumber][jointNumber], *cov[bodyNumber][jointNumber], true, z, false, newCov);
//							*cov[bodyNumber][jointNumber] = newCov;
					}
					else
					{
						// Perform the Kalman prediction step only without using any measurements
						*x[bodyNumber][jointNumber] = filter[bodyNumber][jointNumber]->Update(*x[bodyNumber][jointNumber], *cov[bodyNumber][jointNumber], false, z, false, newCov);
//							*cov[bodyNumber][jointNumber] = newCov;
					}
				}

				// Output the velocities
				DomainHumanTracking::CommBodyData newBodyData = thisSensorData->getBodyDataElemAtPos(bodyNumber);
				DomainHumanTracking::CommJointData newJointData = newBodyData.getJointDataElemAtPos(jointNumber);
				DomainHumanTracking::Comm3dVector velocities;
				velocities.setX(x[bodyNumber][jointNumber]->GetValue(3));
				velocities.setY(x[bodyNumber][jointNumber]->GetValue(4));
				velocities.setZ(x[bodyNumber][jointNumber]->GetValue(5));
				newJointData.setJointVelocity3d(velocities);

				// Filter out any joint velocities that exceeds the max by setting the joint to INFERRED
				if (
						(
								x[bodyNumber][jointNumber]->GetValue(3) * x[bodyNumber][jointNumber]->GetValue(3) +
								x[bodyNumber][jointNumber]->GetValue(4) * x[bodyNumber][jointNumber]->GetValue(4) +
								x[bodyNumber][jointNumber]->GetValue(5) * x[bodyNumber][jointNumber]->GetValue(5)
						)
						> (maximumVelocity * maximumVelocity)
				)
				{
					newJointData.setIsJointTracked(DomainHumanTracking::JointTrackingStateType::INFERRED);
				}

				newBodyData.setJointDataElemAtPos(jointNumber, newJointData);
				thisSensorData->setBodyDataElemAtPos(bodyNumber, newBodyData);

				lastMeasurementTime[bodyNumber][jointNumber] = convertToUsecs(thisSensorData->getBodyDataElemAtPos(0).getAcquisitionTime()) / 1000000.0;
			}
			else
			{
				DomainHumanTracking::Comm3dVector jointVelocity = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber).getJointVelocity3d();
				jointVelocity.setX(0.0);
				jointVelocity.setY(0.0);
				jointVelocity.setZ(0.0);
				DomainHumanTracking::CommJointData jointData = thisSensorData->getBodyDataElemAtPos(bodyNumber).getJointDataElemAtPos(jointNumber);
				jointData.setJointVelocity3d(jointVelocity);
				DomainHumanTracking::CommBodyData bodyData = thisSensorData->getBodyDataElemAtPos(bodyNumber);
				bodyData.setJointDataElemAtPos(jointNumber, jointData);
				thisSensorData->setBodyDataElemAtPos(bodyNumber, bodyData);
			}
		}
	}
}

long long int FilterAndTrack::convertToUsecs(CommBasicObjects::CommTimeStamp timeStamp)
{
	return (timeStamp.getSec() * 1000000ll) + timeStamp.getUsec();
}

void FilterAndTrack::display(const std::shared_ptr<std::vector<std::shared_ptr<op::Datum>>>& datumsPtr, cv::Mat& depthImage)
{
    try
    {
        if (datumsPtr != nullptr && !datumsPtr->empty())
        {
            // Display image
            const cv::Mat cvColourMat = OP_OP2CVCONSTMAT(datumsPtr->at(0)->cvOutputData);
            if (!cvColourMat.empty())
            {
                cv::imshow("Augmented colour input image", cvColourMat);
            }
        }
        else
        {
            std::cout << "Nullptr or empty datumsPtr found when displaying images." << std::endl;
        }

        cv::imshow("Augmented depth input image", depthImage);

        cv::waitKey(1);
    }
    catch (const std::exception& e)
    {
        std::cout << "Exception received when displaying images." << std::endl;
    }
}

